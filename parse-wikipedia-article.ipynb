{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and denormalization of Wikipedia articles for fragmented indexing in Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from elasticsearch import Elasticsearch\n",
    "import wikipediaapi\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yake\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': '1588549463', 'timestamp': '23:44:23', 'count': '28077'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.cat.count(\"wikipedia\", params={\"format\": \"json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all indices if need to reindex\n",
    "# client.indices.delete(\"_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "### Load all article titles to download from articles_to_download.txt\n",
    "\n",
    "I already have a list of wikipedia articles (articles.txt) that I will process and index in Elasticsearch. It is a list of dictionaries with wikipedia page ID as keys and the aricle titles as values.\n",
    "\n",
    "The wikipedia articles to be downloaded belong to the following categories:\n",
    "```\n",
    "categories = ['Presidents of the United States', \n",
    "              'Marvel Comics', \n",
    "              'American comics writers',\n",
    "              'Marvel Comics editors-in-chief',\n",
    "              'Machine learning',\n",
    "              'Natural language processing',\n",
    "              'Coronaviridae',\n",
    "              '21st-century American comedians',\n",
    "              'Pandemics']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the artclie key-values pairs\n",
    "with open('articles_to_download.txt') as json_file:\n",
    "    articles = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a list of dictionaries with section text\n",
    "\n",
    "- Break down the articles into list of dictionaries, where each dictionary is a section. \n",
    "- Build dataframe with columns \"level\", \"text\" and \"section_title\"\n",
    "- Create new fields \"article_title\", \"main_section\", \"tags\" and \"subsection\"\n",
    "- Reconstruct all section titles for nested documents. Wikipedia-API allows to access all sections, but it does not allow easily get all levels of titles for nested articles(e.g., section--> subsection-->sub-subsection)\n",
    "- Clean dataset: remove redundant columns (\"level\" and \"subsection\") and any blank sections with an empty \"text\" field, (some higher level sections with multiple subsections tend to be empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconstruct_article(page):\n",
    "    section_list = []\n",
    "            \n",
    "    section_list = [{'level': 0,\n",
    "                     'section_title': 'Summary',\n",
    "                     'text': page.summary}]\n",
    "\n",
    "    def get_sections(sections, level=0):\n",
    "        for s in sections:\n",
    "            section_dict = {'level':level,\n",
    "                            'section_title': s.title, \n",
    "                            'text': s.text}\n",
    "            section_list.append(section_dict)\n",
    "            get_sections(s.sections, level + 1)\n",
    "\n",
    "    get_sections(page.sections)\n",
    "    return section_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents(page, section_list):\n",
    "    \n",
    "    # Transform list of dictionaries to dataframe\n",
    "    df = pd.DataFrame(section_list)\n",
    "    \n",
    "    # Create column \"main_section\"\n",
    "    df['main_section'] = np.nan\n",
    "    df.loc[df['level']==0, 'main_section'] = df['section_title']\n",
    "    df['main_section'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Create column \"subsection\"\n",
    "    df['subsection'] = np.nan\n",
    "    df.loc[df['text']=='', 'subsection'] = df['section_title']\n",
    "    df['subsection'].fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Add wikipedia article title, source url and page id\n",
    "    df1 = df.replace(np.nan, '', regex=True)\n",
    "    df1['article_title']=page.title\n",
    "    df1['source_url']=page.fullurl\n",
    "    df1['page_id']=page.pageid\n",
    "    \n",
    "    # Create a list of section tags\n",
    "    df1['tags']=df1.apply(lambda row: [row['article_title'],\n",
    "                                       row['main_section'], \n",
    "                                       row['subsection'], \n",
    "                                       row['section_title']],\n",
    "                          axis=1)\n",
    "    df1['tags']=df1['tags'].apply(lambda cell: [s for s in cell if s!=\"\"])\n",
    "    df1['tags']=df1['tags'].apply(lambda cell: list(dict.fromkeys(cell)))\n",
    "    \n",
    "    # Drop rows with NaN values (empty sections)\n",
    "    df2 = df1.replace('', np.nan, regex=True)\n",
    "    df2 = df2.drop(['level', 'subsection'], axis=1).dropna()\n",
    "    \n",
    "    # Transform a list of tags to a comma separated string\n",
    "    df2['tags']=df2.apply(lambda row: ','.join(row['tags']),axis=1)\n",
    "    \n",
    "    # Add tags as the first line of the text field\n",
    "    df2['text']=df2.apply(lambda row: row['tags']+'\\n'+row['text'],axis=1)\n",
    "    \n",
    "    # Add number of section withing the article\n",
    "    df2['section_number']=df2.index\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a list of dataframes. Each dataframe on the list has all the sections of an individual article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong when loading -1 article 'fullurl'\n",
      "Something went wrong when loading -1 article 'fullurl'\n",
      "Something went wrong when loading -1 article 'fullurl'\n"
     ]
    }
   ],
   "source": [
    "article_df_list = []\n",
    "\n",
    "for key, value in articles.items():\n",
    "    try:\n",
    "        page = wiki_wiki.page(value)\n",
    "        sections = deconstruct_article(page)\n",
    "        document_df = build_documents(page, sections)\n",
    "        article_df_list.append(document_df)\n",
    "    except Exception as error:\n",
    "        print(f\"Something went wrong when loading {page.pageid} article\",error)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all dataframes with individual articles into a single dataframe\n",
    "final_df = pd.concat(article_df_list)\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save \n",
    "final_df.to_pickle(\"data/articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(\"./articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load articles from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_articles = unpickled_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding other fields to the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add keywords with YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor()\n",
    "def extract_keywords(text):\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kw = ','.join([kw[0] for kw in keywords])\n",
    "    return kw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df['keywords']=''\n",
    "unpickled_df['keywords'] = unpickled_df.apply(lambda x: extract_keywords(x['text']) if len(x['text'])>1000 else \"\", axis=1)\n",
    "unpickled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df.to_pickle(\"./articles_with_keywords.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all wikipedia categories (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categores\n",
    "def print_categories(page):\n",
    "    category_list = []\n",
    "    categories = page.categories\n",
    "    for title in sorted(categories.keys()):\n",
    "        if 'articles' not in str(categories[title]) and 'pages' not in str(categories[title]):\n",
    "            category_list.append(str(categories[title]).split(':')[1].split('(i')[0].strip())\n",
    "    return category_list\n",
    "\n",
    "categories = print_categories(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate ElasticSearch database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_with_kw = unpickled_df.to_dict('records')\n",
    "articles_with_kw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in articles_with_kw:\n",
    "    try:\n",
    "        client.index(index='wikipedia', body=item)\n",
    "    except Exception as error:\n",
    "        pageid = item['page_id']\n",
    "        print(f\"Something went wrong with {pageid}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update all documents in elasticsearch\n",
    "\n",
    "Add new field to every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.get_mapping('wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.put_mapping(index=\"wikipedia\", body= {\"properties\": {\"keywords\": {\"type\": \"text\"}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_by_id = {\"size\": 100, \"query\": {\"term\": {\"page_id\": 20966}}}\n",
    "\n",
    "# get a response using the Search API\n",
    "response = client.search(index=\"wikipedia\", body=query_by_id)\n",
    "documents = response['hits']['hits']\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the list of documents\n",
    "\n",
    "source_to_update = {\"_source\" : {\"keywords\": kw}}\n",
    "for num, doc_id in enumerate(documents):\n",
    "    \n",
    "    if len(doc_id[\"_source\"][\"text\"]) > 1000:\n",
    "        keywords = kw_extractor.extract_keywords(doc_id[\"_source\"][\"text\"])\n",
    "        kw = ','.join([kw[0] for kw in keywords])\n",
    "        title = doc_id[\"_source\"][\"section_title\"]\n",
    "        l = len(doc_id[\"_source\"][\"text\"])\n",
    "        print(f\"Result {num}: {title} of length {l}\")\n",
    "        print(type(kw))\n",
    "    \n",
    "    # catch API errors\n",
    "        try:\n",
    "            # call the Update method\n",
    "            response = client.update(index=\"wikipedia\", doc_type=\"_doc\", id=doc_id[\"_id\"], body=source_to_update)\n",
    "            print(\"success!\")\n",
    "        except Exception as error:\n",
    "            print(\"something went wrong\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find keywords with YAKE\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keywords(article_sections):\n",
    "    for s in article_sections:\n",
    "        keywords = kw_extractor.extract_keywords(s['text'])\n",
    "        kw = ','.join([kw[0] for kw in keywords])\n",
    "        s['keywords']=kw\n",
    "    return article_sections\n",
    "\n",
    "sections_with_kw = add_keywords(article_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sections_with_kw)\n",
    "pprint(sections_with_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in article_sections:\n",
    "    keywords = kw_extractor.extract_keywords(s['text'])\n",
    "    kw = ','.join([kw[0] for kw in keywords])\n",
    "    s['keywords']=kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(article_sections[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "question = \"When Turing test was proposed?\"\n",
    "\n",
    "body = {    \n",
    "    \"query\": {\n",
    "        \"bool\" : {\n",
    "          \"must_not\" : [\n",
    "            {\"term\" : { \"section_title\" : \"lists\" }},\n",
    "            {\"term\" : { \"section_title\" : \"links\" }},\n",
    "            {\"term\" : { \"section_title\" : \"other\" }},\n",
    "            {\"term\" : { \"section_title\" : \"see also\" }},\n",
    "            {\"term\" : { \"section_title\" : \"bibliography\" }},\n",
    "            {\"term\" : { \"section_title\" : \"references\" }},\n",
    "            {\"term\" : { \"section_title\" : \"official\" }},\n",
    "            {\"match\" : { \"tags\" : \"see also\" }},\n",
    "              \n",
    "          ],\n",
    "          \"should\": [\n",
    "              {\"multi_match\" : {\"query\":question, \n",
    "                                \"fields\": [ \"keywords^3\", \"text\" ] }},\n",
    "#               {\"range\" : {\n",
    "#                 \"section_number\" : {\n",
    "#                 \"gte\" : 0,\n",
    "#                 \"lte\" : 3,\n",
    "#                 \"boost\" : 5\n",
    "#             }}}\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "    \"highlight\": {\n",
    "        \"fields\": {\n",
    "            \"text\": {\"number_of_fragments\": 3, 'order': \"score\", \"fragment_size\": 512}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "}\n",
    "docs = client.search(body, index=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {question}\")\n",
    "print(\"\")\n",
    "print(\"Search results:\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "for i, doc in enumerate(docs[\"hits\"][\"hits\"]):\n",
    "    title = doc['_source']['article_title']\n",
    "    section_title = doc['_source']['section_title']\n",
    "    score = doc['_score']\n",
    "    snippet = doc['highlight']['text'][0]\n",
    "    snippet_soup = BeautifulSoup(snippet)\n",
    "    print(f'Result {i}: {title} | {section_title} | Relevance score {score}')\n",
    "    print(snippet_soup.get_text())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*string_query* returns documents based on a provided query string, using a parser with a strict syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Barack Obama AND Michelle Obama\"\n",
    "\n",
    "body = {\n",
    "    \"query\": {\n",
    "        \"query_string\" : {\n",
    "            \"query\" : question,\n",
    "            \"default_field\" : \"text\"\n",
    "        }\n",
    "    },\n",
    "    \"highlight\": {\n",
    "        \"fields\": {\n",
    "            \"text\": {\"number_of_fragments\": 3, 'order': \"score\", \"fragment_size\": 512}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "docs = client.search(body, index=\"\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {question}\")\n",
    "print(\"\")\n",
    "print(\"Search results:\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "for i, doc in enumerate(docs[\"hits\"][\"hits\"]):\n",
    "    title = doc['_source']['article_title']\n",
    "    section_title = doc['_source']['section_title']\n",
    "    score = doc['_score']\n",
    "    snippet = doc['highlight']['text'][0]\n",
    "    snippet_soup = BeautifulSoup(snippet)\n",
    "    print(f'Result {i}: {title} | {section_title} | Relevance score {score}')\n",
    "    print(snippet_soup.get_text())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns documents based on a provided query string, using a parser with a limited but fault-tolerant syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is 'Stan Lee'?\"\n",
    "\n",
    "body = {\n",
    "  \"query\": {\n",
    "    \"simple_query_string\" : {\n",
    "        \"query\": question,\n",
    "        \"fields\": [\"keywords^5\", \"text\"],\n",
    "#         \"default_operator\": \"and\",\n",
    "        \"auto_generate_synonyms_phrase_query\" : False\n",
    "    }\n",
    "  },\n",
    "    \"highlight\": {\n",
    "        \"fields\": {\n",
    "            \"text\": {\"number_of_fragments\": 3, 'order': \"score\", \"fragment_size\": 512}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "docs = client.search(body, index=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Question: {question}\")\n",
    "print(\"\")\n",
    "print(\"Search results:\")\n",
    "print(\"----------------------\")\n",
    "\n",
    "for i, doc in enumerate(docs[\"hits\"][\"hits\"]):\n",
    "    title = doc['_source']['article_title']\n",
    "    section_title = doc['_source']['section_title']\n",
    "    score = doc['_score']\n",
    "    snippet = doc['highlight']['text'][0]\n",
    "    snippet_soup = BeautifulSoup(snippet)\n",
    "    print(f'Result {i}: {title} | {section_title} | Relevance score {score}')\n",
    "    print(snippet_soup.get_text())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbooks] *",
   "language": "python",
   "name": "conda-env-nbooks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
