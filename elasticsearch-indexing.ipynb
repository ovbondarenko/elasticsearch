{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from elasticsearch import Elasticsearch\n",
    "import wikipediaapi\n",
    "from slugify import slugify\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with _indices API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural-language-processing': {'aliases': {}},\n",
       " 'marvel-comics': {'aliases': {}},\n",
       " 'pandemics': {'aliases': {}},\n",
       " 'presidents-of-the-united-states': {'aliases': {}},\n",
       " 'coronaviridae': {'aliases': {}},\n",
       " 'machine-learning': {'aliases': {}},\n",
       " '21st-century-american-comedians': {'aliases': {}},\n",
       " 'american-comics-writers': {'aliases': {}},\n",
       " 'marvel-comics-editors-in-chief': {'aliases': {}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check database for existing indices\n",
    "client.indices.get_alias(\"_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coronaviridae': {'mappings': {'properties': {'page_id': {'type': 'long'},\n",
      "                                               'source': {'type': 'text'},\n",
      "                                               'text': {'properties': {'section_content': {'type': 'text'},\n",
      "                                                                       'section_num': {'type': 'integer'},\n",
      "                                                                       'section_title': {'type': 'text'}},\n",
      "                                                        'type': 'nested'},\n",
      "                                               'title': {'type': 'text'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Check an index mapping\n",
    "pprint(client.indices.get_mapping(\"coronaviridae\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'american-comics-writers': {'mappings': {'properties': {'page_id': {'type': 'long'},\n",
      "                                                         'source': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                                           'type': 'keyword'}},\n",
      "                                                                    'type': 'text'},\n",
      "                                                         'text': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                                         'type': 'keyword'}},\n",
      "                                                                  'type': 'text'},\n",
      "                                                         'title': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                                          'type': 'keyword'}},\n",
      "                                                                   'type': 'text'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Check an index mapping\n",
    "pprint(client.indices.get_mapping(\"american-comics-writers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete al documents in specified indices. Can be a list. \n",
    "client.indices.delete(index=\"american-comics-writers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.get_alias(\"_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=[\"natural-languge-processing\", \"presidents-of-the-united-states\"])\n",
    "client.indices.get_alias(\"_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(\"_all\")\n",
    "client.indices.get_alias(\"_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new indices and documents\n",
    "\n",
    "Indices = collections of documents. Indices can be created before the documents are added to them.\n",
    "\n",
    "### Collect Wikipedia articles\n",
    "We pull all wikipedia articles from selected categories from wikipedia to create our database of artices in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_wiki = wikipediaapi.Wikipedia('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_categorymembers(categorymembers, level=0, max_level=2):\n",
    "#         for c in categorymembers.values():\n",
    "#             print(\"%s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n",
    "#             if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "#                 print_categorymembers(c.categorymembers, level=level + 1, max_level=max_level)\n",
    "\n",
    "\n",
    "# cat = wiki_wiki.page(f\"Category:{'Pandemics'}\")\n",
    "# print_categorymembers(cat.categorymembers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a few collections (indices) with default mapping\n",
    "\n",
    "In this case mapping does not have to be specified. Elaticsearch will automatically data types to the document fields and will choose the number of shards in settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Presidents of the United States', \n",
    "                 'Marvel Comics', \n",
    "                 'American comics writers',\n",
    "                 'Marvel Comics editors-in-chief']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find wiki articles in certain category\n",
    "# if category exists, create index with mapping (alsi need a more flexible function)\n",
    "# Take the wiki articles from category and build document (create class)\n",
    "# Insert document into the index\n",
    "\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.title = ''\n",
    "        self.page_id = None\n",
    "        self.source = ''\n",
    "        self.text = ''\n",
    "        \n",
    "    def if_exists(self, page_id, index=\"\"):\n",
    "        return client.search(index=index, \n",
    "                             body={\"query\": \n",
    "                                   {\"match\": \n",
    "                                    {\"page_id\": page_id}\n",
    "                                   }})['hits']['total']['value']\n",
    "        \n",
    "    def insert(self, title, page_id, url, text, index):\n",
    "        self.title=title\n",
    "        self.page_id=page_id\n",
    "        self.source=url\n",
    "        self.text=text\n",
    "        self.body = {'title': self.title,\n",
    "            'page_id': self.page_id,\n",
    "            'source':self.source,\n",
    "            'text': self.text}\n",
    "        \n",
    "        if if_exists(self.page_id) == 0:\n",
    "        \n",
    "            try:\n",
    "                client.index(index=index, body=self.body)\n",
    "                print(f\"Sucess! The article {self.title} was added to index {index}\")\n",
    "            except error:\n",
    "                print(\"Something went wrong\", error)\n",
    "                \n",
    "        else:\n",
    "            print(\"This article is already in the database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_wiki_doc(category):\n",
    "    \n",
    "    if type(category) is not list: category = [ category ]\n",
    "\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "    \n",
    "    for c in category:\n",
    "\n",
    "        cat = wiki_wiki.page(f\"Category:{c}\")\n",
    "\n",
    "        for key in cat.categorymembers.keys():\n",
    "            page = wiki_wiki.page(key)\n",
    "\n",
    "            if not \"Category:\" in page.title:\n",
    "                \n",
    "                doc = Document()\n",
    "                doc.insert(page.title, page.pageid, page.fullurl, page.text, index=slugify(c))\n",
    "                \n",
    "            break\n",
    "                \n",
    "simple_wiki_doc(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new index for nested data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \n",
    "            \"text\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\":{\n",
    "                    \"section_num\": {\"type\":\"integer\"},\n",
    "                    \"section_title\": {\"type\":\"text\"},\n",
    "                    \"section_content\": {\"type\":\"text\"}\n",
    "                }\n",
    "            },\n",
    "        \n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        \n",
    "            \"source\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        \n",
    "            \"page_id\": {\n",
    "                \"type\": \"long\"\n",
    "            },\n",
    "            \n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing indexing  - mappings\n",
    "\n",
    "Qary chatbot uses BERT or ALBERT model to generate answers from context.\n",
    "For it to perform better we want to only give it shorter and most relevant context. Storing articles in sections may improve performance\n",
    "\n",
    "To store wikipedia artilcles in sections, we define the text field as nested datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories2 = ['Machine learning',\n",
    "              'Natural language processing',\n",
    "              'Coronaviridae',\n",
    "              '21st-century American comedians',\n",
    "              'Pandemics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \n",
    "            \"text\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\":{\n",
    "                    \"section_num\": {\"type\":\"integer\"},\n",
    "                    \"section_title\": {\"type\":\"text\"},\n",
    "                    \"section_content\": {\"type\":\"text\"}\n",
    "                }\n",
    "            },\n",
    "        \n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        \n",
    "            \"source\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        \n",
    "            \"page_id\": {\n",
    "                \"type\": \"long\"\n",
    "            },\n",
    "            \n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some new indices with nested data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(article):\n",
    "    \n",
    "    text = article.text\n",
    "    section_titles = [sec.title for sec in article.sections]\n",
    "    \n",
    "    sections = [{'section_num': 0},\n",
    "                {'section_title': \"Summary\"},\n",
    "                {'section_content': article.summary}]\n",
    "    \n",
    "    for i, title in enumerate(section_titles[::-1]):\n",
    "        num = len(section_titles)-i\n",
    "        if len(text.split(f\"\\n\\n{title}\")) == 2:\n",
    "            section_dict = {\"section_num\": num,\n",
    "                            \"section_title\": title,\n",
    "                            \"section_content\": text.split(f\"\\n\\n{title}\")[-1]}\n",
    "            sections.append(section_dict)\n",
    "            text = text.split(f\"\\n\\n{title}\")[0]\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_insert_wiki(category, mapping):\n",
    "    \n",
    "    if type(category) is not list: category = [ category ]\n",
    "\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "    \n",
    "    for c in category:\n",
    "        \n",
    "        \n",
    "        '''Create and empty index with predefined data structure'''\n",
    "        try:\n",
    "            client.indices.create(index=slugify(c), body={\"mappings\":mapping})\n",
    "            print(f\"Index {c} has been created\")\n",
    "            \n",
    "        except error:\n",
    "            print(\"something went wrong\", error)\n",
    "            \n",
    "\n",
    "        '''Access the list of wikipedia articles in category c'''\n",
    "        cat = wiki_wiki.page(f\"Category:{c}\")\n",
    "\n",
    "        for key in cat.categorymembers.keys():\n",
    "            page = wiki_wiki.page(key)\n",
    "\n",
    "            if not \"Category:\" in page.title:\n",
    "                ''' Build a dictionary and add in to the index'''\n",
    "                \n",
    "                text = parse_article(page)\n",
    "                doc = Document()\n",
    "                doc.insert(page.title, page.pageid, page.fullurl, text, index=slugify(c))\n",
    "                \n",
    "search_insert_wiki(categories2, mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebooks] *",
   "language": "python",
   "name": "conda-env-notebooks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
